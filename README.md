# GPT-Scratch

### What is this?
This repo is my attempt at a rough implementation of nanoGPT. I train on a dataset of 30,000 unique Twitter usernames that mainly focus on tech and anime.

### Why?
The primary motivation behind this project was to gain a deeper understanding of transformer models by implementing one from scratch. Training models on traditional datasets like Shakespeare's works sounded boring, so to make things interesting I decided to create my own  dataset by scraping Twitter usernames, particularly those related to tech and anime.

Additionally, I needed a new Twitter username for myself anyway Lol
### todo

- [x] PyTorch
- [ ] tinygrad
- [ ] JAX


### References
Awesome Transformers Resources
- [Formal Algorithms for Transformers](https://arxiv.org/abs/2207.09238)
- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)
- [Master Positional Encoding](https://towardsdatascience.com/master-positional-encoding-part-i-63c05d90a0c3)
- [Let's build the GPT Tokenizer](https://www.youtube.com/watch?v=zduSFxRajkE&t=3853s&ab_channel=AndrejKarpathy)
- [Let's build GPT: from scratch, in code, spelled out.](https://www.youtube.com/watch?v=kCc8FmEb1nY&t=5463s&ab_channel=AndrejKarpathy)

